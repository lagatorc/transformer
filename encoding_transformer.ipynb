{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_dot_product_attention(Q, K, V, mask):\n",
    "    \"\"\"\n",
    "    Calculates the dot product attention weights\n",
    "    and returns a matrix Z, which is the same size as \n",
    "    Q, K, and V.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    Q - The result of applying Wq to X. \n",
    "      - Shape (..., sequence_len, dim_Wq)\n",
    "    K - The result of applying Wk to X. \n",
    "      - Shape (..., sequence_len, dim_Wk)\n",
    "    V - The result of applying Wv to X. \n",
    "      - Shape (..., sequence_len, dim_Wv)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    Z - The matrix created by applying the scaled attention \n",
    "            weights to the V matrix.\n",
    "      - Shape (..., sequence_len, model_dim)\n",
    "      \n",
    "    \"\"\"\n",
    "    #Normalizing Q\n",
    "    Q = tf.divide(Q, tf.norm(Q, axis=-1, keepdims=True))\n",
    "    \n",
    "    #Normalizing K\n",
    "    K = tf.divide(K, tf.norm(K, axis=-1, keepdims=True))\n",
    "    \n",
    "    #compute the dot product of all query and key vectors (b/c they are normalized 0 >= values <=1\n",
    "    attention_logits = tf.matmul(Q, K, transpose_b=True)\n",
    "    \n",
    "    attention_logits *= 1e2\n",
    "        \n",
    "    if mask is not None:\n",
    "        attention_logits += (mask * -1e9)\n",
    "    #apply softmax to find the weights\n",
    "    attention_weights = tf.nn.softmax(attention_logits, axis=-1)\n",
    "    #multiply the weights by the Value matrix\n",
    "    Z = tf.matmul(attention_weights, V)\n",
    "    \n",
    "    return Z, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_heads, embedding_dim):\n",
    "        super(MHAttention, self).__init__()\n",
    "        assert embedding_dim % num_heads == 0\n",
    "\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.Wq = tf.keras.layers.Dense(self.embedding_dim)\n",
    "        self.Wk = tf.keras.layers.Dense(self.embedding_dim)\n",
    "        self.Wv = tf.keras.layers.Dense(self.embedding_dim)\n",
    "\n",
    "        self.Wz = tf.keras.layers.Dense(self.embedding_dim)\n",
    "        \n",
    "    def create_heads(self, x, batch_size):\n",
    "        \n",
    "        return tf.reshape(tf.transpose(x), (batch_size, self.num_heads, -1, self.head_dim))\n",
    "        \n",
    "    def call(self, q, k, v, mask):\n",
    "         \n",
    "        batch_size = q.shape[0]\n",
    "        \n",
    "        q = self.Wq(q)\n",
    "        k = self.Wk(k)\n",
    "        v = self.Wv(v)\n",
    "        \n",
    "        q = self.create_heads(q, batch_size)\n",
    "        k = self.create_heads(k, batch_size)\n",
    "        v = self.create_heads(v, batch_size)\n",
    "        \n",
    "        z, attention_weights = normalized_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        concat_z = tf.transpose(z, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_z = tf.reshape(concat_z, (batch_size, -1, self.embedding_dim))\n",
    "        \n",
    "        z = self.Wz(concat_z)\n",
    "        \n",
    "        return z, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(embedding_dim, ff_hidden_dim):\n",
    "    \n",
    "    hidden_layer = tf.keras.layers.Dense(ff_hidden_dim, activation='relu') #(batch size, seq len, hidden dim)\n",
    "    output_layer = tf.keras.layers.Dense(embedding_dim) #(batch size, seq len, embedding dim)\n",
    "    \n",
    "    return tf.keras.Sequential([\n",
    "        hidden_layer,\n",
    "        output_layer\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, embedding_dim, num_heads, ff_hidden_dim, dropout_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.mha = MHAttention(num_heads, embedding_dim)\n",
    "        \n",
    "        self.ff = feed_forward(embedding_dim, ff_hidden_dim)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, input_tensor, training):\n",
    "        \n",
    "        #Sublayer 1\n",
    "        mha_output, _ = self.mha(input_tensor)\n",
    "        mha_output = self.dropout1(mha_output, training=training)\n",
    "        sublayer_1_output = self.layernorm1(mha_output + input_tensor)\n",
    "        \n",
    "        #Sublayer 2\n",
    "        ff_output = self.ff(sublayer_1_output)\n",
    "        ff_output = self.dropout2(ff_output, training=training)\n",
    "        return self.layernorm2(ff_output + sublayer_1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_encoders, embedding_dim, num_heads, ff_hidden_dim, dropout = 0.1):\n",
    "        super(EncoderStack, self).__init__()\n",
    "        \n",
    "        self.num_encoders = num_encoders\n",
    "        \n",
    "        self.encoders = []\n",
    "        for i in range(self.num_encoders):\n",
    "            self.encoders.append(Encoder(embedding_dim, num_heads, ff_hidden_dim))\n",
    "            \n",
    "    def call(self, input_tensor, training):\n",
    "        \n",
    "        output_tensor = input_tensor\n",
    "        \n",
    "        for i in range(self.num_encoders):\n",
    "            output_tensor = self.encoders[i](output_tensor, training)\n",
    "            \n",
    "        return output_tensor    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderStack(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_encoders, embedding_dim, num_heads, ff_hidden_dim, vocab_size, dropout=0.1):\n",
    "        super(TransformerEncoderStack, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = positional_encoding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.encoders = EncoderStack(num_encoders, embedding_dim, num_heads, ff_hidden_dim)\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, input_tensor, training):\n",
    "        \n",
    "        print(f'input shape:{input_tensor.shape}')\n",
    "        \n",
    "        max_sequence_length = input_tensor.shape[1]\n",
    "        \n",
    "        embeddings = self.embedding(input_tensor)\n",
    "        print(f'embedding_shape:{embeddings.shape}')\n",
    "        \n",
    "        embeddings += self.positional_encoding[:, :max_sequence_length, :]\n",
    "        print(self.positional_encoding.shape)\n",
    "        \n",
    "        embeddings = self.dropout(embeddings, training=training)\n",
    "        \n",
    "        output = self.encoders(embeddings, training)\n",
    "        print(embeddings.shape)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:(5, 30)\n",
      "embedding_shape:(5, 30, 128)\n",
      "(1, 30, 128)\n",
      "(5, 30, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 30, 128])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.random.uniform((5, 30)) #this would be an input of 5 sentences with a max input length of 30\n",
    "tes = TransformerEncoderStack(2, 128, 4, 512, 30)\n",
    "out = tes(y, False)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
