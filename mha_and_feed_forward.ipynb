{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_dot_product_attention(Q, K, V, mask):\n",
    "    \"\"\"\n",
    "    Calculates the dot product attention weights\n",
    "    and returns a matrix Z, which is the same size as \n",
    "    Q, K, and V.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    Q - The result of applying Wq to X. \n",
    "      - Shape (..., sequence_len, dim_Wq)\n",
    "    K - The result of applying Wk to X. \n",
    "      - Shape (..., sequence_len, dim_Wk)\n",
    "    V - The result of applying Wv to X. \n",
    "      - Shape (..., sequence_len, dim_Wv)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    Z - The matrix created by applying the scaled attention \n",
    "            weights to the V matrix.\n",
    "      - Shape (..., sequence_len, model_dim)\n",
    "      \n",
    "    \"\"\"\n",
    "    #Normalizing Q\n",
    "    Q = tf.divide(Q, tf.norm(Q, axis=-1, keepdims=True))\n",
    "    \n",
    "    #Normalizing K\n",
    "    K = tf.divide(K, tf.norm(K, axis=-1, keepdims=True))\n",
    "    \n",
    "    #compute the dot product of all query and key vectors (b/c they are normalized 0 >= values <=1\n",
    "    attention_logits = tf.matmul(Q, K, transpose_b=True)\n",
    "    \n",
    "    attention_logits *= 1e2\n",
    "    \n",
    "    attention_logits += (mask * -1e9)\n",
    "        \n",
    "    #apply softmax to find the weights\n",
    "    attention_weights = tf.nn.softmax(attention_logits, axis=-1)\n",
    "    \n",
    "    #multiply the weights by the Value matrix\n",
    "    Z = tf.matmul(attention_weights, V)\n",
    "    \n",
    "    return Z, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_heads, embedding_dim):\n",
    "        super(MHAttention, self).__init__()\n",
    "        assert embedding_dim % num_heads == 0\n",
    "\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.Wq = tf.keras.layers.Dense(self.embedding_dim)\n",
    "        self.Wk = tf.keras.layers.Dense(self.embedding_dim)\n",
    "        self.Wv = tf.keras.layers.Dense(self.embedding_dim)\n",
    "\n",
    "        self.Wz = tf.keras.layers.Dense(self.embedding_dim)\n",
    "        \n",
    "    def create_heads(self, x, batch_size):\n",
    "        \n",
    "        return tf.reshape(tf.transpose(x), (batch_size, self.num_heads, -1, self.head_dim))\n",
    "        \n",
    "    def call(self, q, k, v):\n",
    "         \n",
    "        batch_size = q.shape[0]\n",
    "        \n",
    "        q = self.Wq(q)\n",
    "        k = self.Wk(k)\n",
    "        v = self.Wv(v)\n",
    "        \n",
    "        q = self.create_heads(q, batch_size)\n",
    "        k = self.create_heads(k, batch_size)\n",
    "        v = self.create_heads(v, batch_size)\n",
    "        \n",
    "        z, attention_weights = normalized_dot_product_attention(q, k, v)\n",
    "        \n",
    "        concat_z = tf.transpose(z, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_z = tf.reshape(concat_z, (batch_size, -1, self.embedding_dim))\n",
    "        \n",
    "        z = self.Wz(concat_z)\n",
    "        \n",
    "        return z, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 60, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MHAttention(num_heads=8, embedding_dim=512)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, y, y)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(embedding_dim, hidden_dim):\n",
    "    \n",
    "    hidden_layer = tf.keras.layers.Dense(hidden_dim, activation='relu') #(batch size, seq len, hidden dim)\n",
    "    output_layer = tf.keras.layers.Dense(embedding_dim) #(batch size, seq len, embedding dim)\n",
    "    \n",
    "    return tf.keras.Sequential([\n",
    "        hidden_layer,\n",
    "        output_layer\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 60, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_ff = feed_forward(512, 1024)\n",
    "temp_ff(y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = tf.constant([[[0, 10, 0, 0, 0, 100, 1000, 0],\n",
    "                  [0, 0,  1, 0, 100, 0, 0, 0],\n",
    "                  [10, 0, 0, 1, 0, 0, 0, 1000],\n",
    "                  [10, 0, 0, 0, 0, 0, 0, 0],\n",
    "                  [10, 0, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 0,  1, 0, 100, 0, 1000, 0]]], dtype=tf.float32)\n",
    "num_heads = 4\n",
    "head_dim = 2\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using 4 heads. So we in theory pass x through 4 different Wq layers, then use the resulting q in self attention. We can actually do this with one layer and split up the result into 4 q's to then be passed into 4 different self attention layers. Below I'm going to try and figure out how to split up the results of one large Wq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6, 8), dtype=float32, numpy=\n",
       "array([[[   0.,   10.,    0.,    0.,    0.,  100., 1000.,    0.],\n",
       "        [   0.,    0.,    1.,    0.,  100.,    0.,    0.,    0.],\n",
       "        [  10.,    0.,    0.,    1.,    0.,    0.,    0., 1000.],\n",
       "        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
       "        [  10.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    1.,    0.,  100.,    0., 1000.,    0.]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_multi_head = tf.reshape(tf.transpose(q), (batch_size, num_heads, -1, head_dim))\n",
    "\n",
    "#The shape is (batch size, number of heads (width of tensor), sequence_length (height of tensor), head_dim (# of features in tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 6, 2), dtype=float32, numpy=\n",
       "array([[[[   0.,    0.],\n",
       "         [  10.,   10.],\n",
       "         [  10.,    0.],\n",
       "         [  10.,    0.],\n",
       "         [   0.,    0.],\n",
       "         [   0.,    0.]],\n",
       "\n",
       "        [[   0.,    1.],\n",
       "         [   0.,    0.],\n",
       "         [   0.,    1.],\n",
       "         [   0.,    0.],\n",
       "         [   1.,    0.],\n",
       "         [   0.,    0.]],\n",
       "\n",
       "        [[   0.,  100.],\n",
       "         [   0.,    0.],\n",
       "         [   0.,  100.],\n",
       "         [ 100.,    0.],\n",
       "         [   0.,    0.],\n",
       "         [   0.,    0.]],\n",
       "\n",
       "        [[1000.,    0.],\n",
       "         [   0.,    0.],\n",
       "         [   0., 1000.],\n",
       "         [   0.,    0.],\n",
       "         [1000.,    0.],\n",
       "         [   0.,    0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_multi_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, attn = normalized_dot_product_attention(q_multi_head, q_multi_head, q_multi_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 6, 2), dtype=float32, numpy=\n",
       "array([[[[nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan]],\n",
       "\n",
       "        [[nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan]],\n",
       "\n",
       "        [[nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan]],\n",
       "\n",
       "        [[nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan]]]], dtype=float32)>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 6, 6), dtype=float32, numpy=\n",
       "array([[[[nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan]]]], dtype=float32)>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self attention with the split heads creates num_heads output. All these are concatenated then passed through a dense layer to convert them back to the original embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6, 4, 2), dtype=float32, numpy=\n",
       "array([[[[   0.,    0.],\n",
       "         [   0.,    1.],\n",
       "         [   0.,  100.],\n",
       "         [1000.,    0.]],\n",
       "\n",
       "        [[  10.,   10.],\n",
       "         [   0.,    0.],\n",
       "         [   0.,    0.],\n",
       "         [   0.,    0.]],\n",
       "\n",
       "        [[  10.,    0.],\n",
       "         [   0.,    1.],\n",
       "         [   0.,  100.],\n",
       "         [   0., 1000.]],\n",
       "\n",
       "        [[  10.,    0.],\n",
       "         [   0.,    0.],\n",
       "         [ 100.,    0.],\n",
       "         [   0.,    0.]],\n",
       "\n",
       "        [[   0.,    0.],\n",
       "         [   1.,    0.],\n",
       "         [   0.,    0.],\n",
       "         [1000.,    0.]],\n",
       "\n",
       "        [[   0.,    0.],\n",
       "         [   0.,    0.],\n",
       "         [   0.,    0.],\n",
       "         [   0.,    0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = tf.transpose(q_multi_head, perm=[0, 2, 1, 3])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6, 8), dtype=float32, numpy=\n",
       "array([[[   0.,    0.,    0.,    1.,    0.,  100., 1000.,    0.],\n",
       "        [  10.,   10.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
       "        [  10.,    0.,    0.,    1.,    0.,  100.,    0., 1000.],\n",
       "        [  10.,    0.,    0.,    0.,  100.,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    1.,    0.,    0.,    0., 1000.,    0.],\n",
       "        [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(temp, (1, -1, 8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
