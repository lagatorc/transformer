{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot product attention (SDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask):\n",
    "    \"\"\"\n",
    "    Calculates the scaled dot product attention weights\n",
    "    and returns a matrix Z, which is the same size as \n",
    "    Q, K, and V.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    Q - The result of applying Wq to X. \n",
    "      - Shape (..., sequence_len, dim_Wq)\n",
    "    K - The result of applying Wk to X. \n",
    "      - Shape (..., sequence_len, dim_Wk)\n",
    "    V - The result of applying Wv to X. \n",
    "      - Shape (..., sequence_len, dim_Wv)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    Z - The matrix created by applying the scaled attention \n",
    "            weights to the V matrix.\n",
    "      - Shape (..., sequence_len, model_dim)\n",
    "      \n",
    "    \"\"\"\n",
    "    #compute the dot product of all query and key vectors\n",
    "    QK_T = tf.matmul(Q, K, transpose_b=True)\n",
    "    \n",
    "    #scale the dot products by the depth of k (number of columns ie tf.shape(k)[-1])\n",
    "    dk = tf.shape(K)[-1]\n",
    "    dk = tf.cast(dk, tf.float32)\n",
    "    scaled_attention_logits = QK_T / tf.sqrt(dk)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "        \n",
    "    #apply softmax to find the weights\n",
    "    scaled_attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    \n",
    "    #multiply the weights by the Value matrix\n",
    "    Z = tf.matmul(scaled_attention_weights, V)\n",
    "    \n",
    "    return Z, scaled_attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sda_results(Q, K, V, mask):\n",
    "    Z, weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "    print(f'SDA Output is:\\n {Z}')\n",
    "    print(f'SDA Weights are:\\n {weights}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product attention (DA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention(Q, K, V, mask):\n",
    "    \"\"\"\n",
    "    Calculates the dot product attention weights\n",
    "    and returns a matrix Z, which is the same size as \n",
    "    Q, K, and V.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    Q - The result of applying Wq to X. \n",
    "      - Shape (..., sequence_len, dim_Wq)\n",
    "    K - The result of applying Wk to X. \n",
    "      - Shape (..., sequence_len, dim_Wk)\n",
    "    V - The result of applying Wv to X. \n",
    "      - Shape (..., sequence_len, dim_Wv)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    Z - The matrix created by applying the scaled attention \n",
    "            weights to the V matrix.\n",
    "      - Shape (..., sequence_len, model_dim)\n",
    "      \n",
    "    \"\"\"\n",
    "    #compute the dot product of all query and key vectors\n",
    "    QK_T = tf.matmul(Q, K, transpose_b=True)\n",
    "    attention_logits = QK_T\n",
    "    \n",
    "    if mask is not None:\n",
    "        attention_logits += (mask * -1e9)\n",
    "        \n",
    "    #apply softmax to find the weights\n",
    "    attention_weights = tf.nn.softmax(attention_logits, axis=-1)\n",
    "    \n",
    "    #multiply the weights by the Value matrix\n",
    "    Z = tf.matmul(attention_weights, V)\n",
    "    \n",
    "    return Z, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_da_results(Q, K, V, mask):\n",
    "    Z, weights = dot_product_attention(Q, K, V, mask)\n",
    "    print(f'DA Output is:\\n {Z}')\n",
    "    print(f'DA Weights are:\\n {weights}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized dot product attention (NDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_dot_product_attention(Q, K, V, mask):\n",
    "    \"\"\"\n",
    "    Calculates the dot product attention weights\n",
    "    and returns a matrix Z, which is the same size as \n",
    "    Q, K, and V.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    Q - The result of applying Wq to X. \n",
    "      - Shape (..., sequence_len, dim_Wq)\n",
    "    K - The result of applying Wk to X. \n",
    "      - Shape (..., sequence_len, dim_Wk)\n",
    "    V - The result of applying Wv to X. \n",
    "      - Shape (..., sequence_len, dim_Wv)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    Z - The matrix created by applying the scaled attention \n",
    "            weights to the V matrix.\n",
    "      - Shape (..., sequence_len, model_dim)\n",
    "      \n",
    "    \"\"\"\n",
    "    #Normalizing Q\n",
    "    Q = tf.divide(Q, tf.norm(Q, axis=-1, keepdims=True))\n",
    "    \n",
    "    #Normalizing K\n",
    "    K = tf.divide(K, tf.norm(K, axis=-1, keepdims=True))\n",
    "    \n",
    "    #compute the dot product of all query and key vectors (b/c they are normalized 0 >= values <=1\n",
    "    attention_logits = tf.matmul(Q, K, transpose_b=True)\n",
    "    \n",
    "    attention_logits *= 1e3\n",
    "    \n",
    "    if mask is not None:\n",
    "        attention_logits += (mask * -1e9)\n",
    "        \n",
    "    #apply softmax to find the weights\n",
    "    attention_weights = tf.nn.softmax(attention_logits, axis=-1)\n",
    "    \n",
    "    #multiply the weights by the Value matrix\n",
    "    Z = tf.matmul(attention_weights, V)\n",
    "    \n",
    "    return Z, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nda_results(Q, K, V, mask):\n",
    "    Z, weights = normalized_dot_product_attention(Q, K, V, mask)\n",
    "    print(f'NDA Output is:\\n {Z}')\n",
    "    print(f'NDA Weights are:\\n {weights}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: small magnitude query key vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "K1 = tf.constant([[0, 1, 0],\n",
    "                 [0, 0, 1],\n",
    "                 [1, 0, 0],\n",
    "                 [1, 0, 0]], dtype=tf.float32)\n",
    "V1 = tf.constant([[1, 0],\n",
    "                 [10, 0],\n",
    "                 [100, 0],\n",
    "                 [5, 0]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5 = tf.constant([[1, 0, 0]], dtype=tf.float32)\n",
    "Q6 = tf.constant([[0, 1, 0]], dtype=tf.float32)\n",
    "Q7 = tf.constant([[0, 0, 1]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query matrix: \n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "SDA Output is:\n",
      " [[35.6015    0.      ]\n",
      " [24.424534  0.      ]\n",
      " [25.895218  0.      ]]\n",
      "SDA Weights are:\n",
      " [[0.17977124 0.17977124 0.32022873 0.32022873]\n",
      " [0.3725572  0.20914762 0.20914762 0.20914762]\n",
      " [0.20914762 0.3725572  0.20914762 0.20914762]]\n",
      "DA Output is:\n",
      " [[39.859756  0.      ]\n",
      " [20.586304  0.      ]\n",
      " [23.290707  0.      ]]\n",
      "DA Weights are:\n",
      " [[0.13447072 0.13447072 0.3655293  0.3655293 ]\n",
      " [0.47536692 0.17487772 0.17487772 0.17487772]\n",
      " [0.17487772 0.47536692 0.17487772 0.17487772]]\n",
      "NDA Output is:\n",
      " [[52.5  0. ]\n",
      " [ 1.   0. ]\n",
      " [10.   0. ]]\n",
      "NDA Weights are:\n",
      " [[0.  0.  0.5 0.5]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  1.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "Q8 = tf.concat([Q5, Q6, Q7], 0)\n",
    "print(f'Query matrix: \\n{Q8}')\n",
    "print_sda_results(Q8, K1, V1, None)\n",
    "print_da_results(Q8, K1, V1, None)\n",
    "print_nda_results(Q8, K1, V1, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: medium magnitude query key vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = tf.constant([[0, 10, 0],\n",
    "                 [0, 0, 10],\n",
    "                 [10, 0, 0],\n",
    "                 [10, 0, 0]], dtype=tf.float32)\n",
    "V = tf.constant([[1, 0],\n",
    "                 [10, 0],\n",
    "                 [100, 0],\n",
    "                 [5, 0]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = tf.constant([[10, 0, 0]], dtype=tf.float32)\n",
    "Q2 = tf.constant([[0, 10, 0]], dtype=tf.float32)\n",
    "Q3 = tf.constant([[0, 0, 10]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query matrix: \n",
      "[[10.  0.  0.]\n",
      " [ 0. 10.  0.]\n",
      " [ 0.  0. 10.]]\n",
      "SDA Output is:\n",
      " [[52.5  0. ]\n",
      " [ 1.   0. ]\n",
      " [10.   0. ]]\n",
      "SDA Weights are:\n",
      " [[4.2166372e-26 4.2166372e-26 5.0000000e-01 5.0000000e-01]\n",
      " [1.0000000e+00 8.4332744e-26 8.4332744e-26 8.4332744e-26]\n",
      " [8.4332744e-26 1.0000000e+00 8.4332744e-26 8.4332744e-26]]\n",
      "Output is:\n",
      " [[52.5  0. ]\n",
      " [ 1.   0. ]\n",
      " [10.   0. ]]\n",
      "Weights are:\n",
      " [[4.2166372e-26 4.2166372e-26 5.0000000e-01 5.0000000e-01]\n",
      " [1.0000000e+00 8.4332744e-26 8.4332744e-26 8.4332744e-26]\n",
      " [8.4332744e-26 1.0000000e+00 8.4332744e-26 8.4332744e-26]]\n",
      "NDA Output is:\n",
      " [[52.5  0. ]\n",
      " [ 1.   0. ]\n",
      " [10.   0. ]]\n",
      "NDA Weights are:\n",
      " [[0.  0.  0.5 0.5]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  1.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "Q4 = tf.concat([Q1, Q2, Q3], 0)\n",
    "print(f'Query matrix: \\n{Q4}')\n",
    "print_sda_results(Q4, K, V)\n",
    "print_sa_results(Q4, K, V)\n",
    "print_nda_results(Q4, K, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: large magnitude query key vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "K2 = tf.constant([[0, 1000, 0],\n",
    "                 [0, 0, 1000],\n",
    "                 [1000, 0, 0],\n",
    "                 [1000, 0, 0]], dtype=tf.float32)\n",
    "V2 = tf.constant([[1, 0],\n",
    "                 [10, 0],\n",
    "                 [100, 0],\n",
    "                 [5, 0]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9 = tf.constant([[1000, 0, 0]], dtype=tf.float32)\n",
    "Q10 = tf.constant([[0, 1000, 0]], dtype=tf.float32)\n",
    "Q11 = tf.constant([[0, 0, 1000]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query matrix: \n",
      "[[1000.    0.    0.]\n",
      " [   0. 1000.    0.]\n",
      " [   0.    0. 1000.]]\n",
      "SDA Output is:\n",
      " [[52.5  0. ]\n",
      " [ 1.   0. ]\n",
      " [10.   0. ]]\n",
      "SDA Weights are:\n",
      " [[0.  0.  0.5 0.5]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  1.  0.  0. ]]\n",
      "Output is:\n",
      " [[52.5  0. ]\n",
      " [ 1.   0. ]\n",
      " [10.   0. ]]\n",
      "Weights are:\n",
      " [[0.  0.  0.5 0.5]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  1.  0.  0. ]]\n",
      "NDA Output is:\n",
      " [[52.5  0. ]\n",
      " [ 1.   0. ]\n",
      " [10.   0. ]]\n",
      "NDA Weights are:\n",
      " [[0.  0.  0.5 0.5]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  1.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "Q12 = tf.concat([Q9, Q10, Q11], axis=0)\n",
    "print(f'Query matrix: \\n{Q12}')\n",
    "print_sda_results(Q12, K2, V2)\n",
    "print_sa_results(Q12, K2, V2)\n",
    "print_nda_results(Q12, K2, V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: larger dimensional vectors (more realistic values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
